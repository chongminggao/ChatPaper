## Paper:1




1. Title: Counterfactual Interactive Recommendation with Transformer-based State Tracker and Reinforcement Learning Planning

2. Authors: Qing Gao, Yiren Wang, Liangxu Liu, and Zhuoren Jiang

3. Affiliation: None

4. Keywords: counterfactual interactive recommendation, Transformer-based state tracker, reinforcement learning, causal user model

5. Url: https://dl.acm.org/doi/pdf/10.1145/3474085.3475561, Github: None

6. Summary:

- (1): This paper proposes a counterfactual interactive recommendation system that estimates the causal effect of item exposure on user satisfaction and uses a Transformer-based state tracker and reinforcement learning planning to improve performance.

- (2): Past methods in recommendation systems often suffer from the problem of data sparsity and the inability to estimate causality, which can lead to suboptimal recommendations. The proposed approach addresses these issues by explicitly modeling the causal effect of item exposure on user satisfaction, using a Transformer-based state tracker to extract relevant information, and using reinforcement learning planning to make optimal recommendations.

- (3): The research methodology used in this paper involves combining a causal user model with a Transformer-based state tracker and reinforcement learning planning to create a counterfactual interactive recommendation system. The causal user model estimates the counterfactual satisfaction of users, the Transformer-based state tracker extracts key information from user and item vectors, and the reinforcement learning planning is used to make optimal recommendations based on the extracted information.

- (4): The proposed method is evaluated on a real dataset collected from a news recommendation system, and results show that it outperforms several baseline methods in terms of click-through rate and user satisfaction.

7. Strengths and Weaknesses:

Strengths:
- The proposed method explicitly models the causal effect of item exposure on user satisfaction, which can lead to more accurate recommendations.
- The use of a Transformer-based state tracker allows for the extraction of relevant information from user and item vectors, leading to more informative recommendations.
- The experimental results show that the proposed method outperforms several baseline methods in terms of click-through rate and user satisfaction.

Weaknesses:
- The paper does not provide a thorough discussion of the limitations of the proposed method and areas for future research.
- The proposed method may be computationally expensive due to the use of reinforcement learning planning, which may not be scalable to larger datasets.
- The use of a real-world dataset from a news recommendation system may limit the generalizability of the proposed method to other types of recommendation systems.
7. Methods:

- (1): The proposed method is based on counterfactual interactive recommendation with a Transformer-based state tracker and reinforcement learning planning. The approach aims to estimate the causal effect of item exposure on user satisfaction, using a causal user model to estimate counterfactual satisfaction, a Transformer-based state tracker to extract relevant information from user and item vectors, and reinforcement learning planning to make optimal recommendations.

- (2): The framework of the proposed approach consists of three stages: pre-learning stage, RL planning stage, and RL evaluation stage. In the pre-learning stage, a user model is learned via supervised learning. In the RL planning stage, the learned user model is used to learn an RL policy that provides counterfactual satisfaction as the reward. In the RL evaluation stage, the policy is evaluated in the real environment.

- (3): The proposed method is evaluated on a real dataset collected from a news recommendation system. Several baseline methods are compared with the proposed approach in terms of click-through rate and user satisfaction.

- (4): The causal user model used in the proposed approach estimates the counterfactual satisfaction of users by estimating the difference in satisfaction between the exposed and unexposed states. The Transformer-based state tracker extracts important information from the user and item vectors, such as user preferences and item features, to make more informative recommendations.

- (5): The reinforcement learning planning used in the proposed approach aims to optimize the policy by maximizing the expected reward, which is defined in terms of counterfactual satisfaction. The policy is learned using a deep Q-network algorithm, which takes the user and item vectors as inputs and outputs a Q-value for each action.

- (6): The experimental results show that the proposed method outperforms several baseline methods in terms of click-through rate and user satisfaction. The proposed method is scalable to larger datasets and can be applied to other types of recommendation systems. Future research may focus on improving the computational efficiency of the proposed method and exploring the generalizability to other domains.





8. Conclusion:

- (1): This piece of work proposes a counterfactual interactive recommendation system that explicitly models the causal effect of item exposure on user satisfaction. The proposed approach uses a Transformer-based state tracker and reinforcement learning planning to make optimal recommendations based on the extracted information.

- (2): Innovation point: The proposed method uses a causal user model to estimate counterfactual satisfaction and a Transformer-based state tracker to extract relevant information from user and item vectors, which leads to more accurate and informative recommendations. Performance: The experimental results show that the proposed method outperforms several baseline methods in terms of click-through rate and user satisfaction. Workload: The use of reinforcement learning planning may make the proposed method computationally expensive, which may limit its scalability to larger datasets. However, the proposed method is scalable to larger datasets and can be applied to other types of recommendation systems. Future research may focus on improving the computational efficiency of the proposed method and exploring the generalizability to other domains.




